{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Binary classification with the perceptron\\n\\nData: Cornell movie review polarity dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" Binary classification with the perceptron\n",
    "\n",
    "Data: Cornell movie review polarity dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# represent text as a set of features\n",
    "def featurizer(data):\n",
    "    # movie data is already tokenized\n",
    "    tokens = data.split(\" \")\n",
    "    counter=Counter()\n",
    "    \n",
    "    # add all features here\n",
    "    counter+=unigramFeatures(tokens)\n",
    "    counter+=biasTerm()\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary indicators for all words present in text\n",
    "def unigramFeatures(tokens):\n",
    "    counter=Counter()\n",
    "    for t in tokens:\n",
    "        # binary indicators\n",
    "        counter[\"UNIGRAM:%s\" % t]=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bias term\n",
    "def biasTerm():\n",
    "    counter=Counter()\n",
    "    counter[\"BIAS\"]=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for an input directory with {pos,neg} subdirectories, read through each file and tranform into a set of features;\n",
    "# split all data into 90% training and 10% development\n",
    "\n",
    "def getData(directory):\n",
    "    # observation parameters (minimum count for a word to be a feature, max number of total features)\n",
    "    maxVocab=10000\n",
    "    minCount=3\n",
    "\n",
    "    docs = {}\n",
    "    labels = {}\n",
    "    totalCounts=Counter()\n",
    "\n",
    "    featureHash={}\n",
    "    featureNames=[]\n",
    "\n",
    "    # read training data and get feature counts and labels for all documents\n",
    "    for label in ['pos', 'neg']:\n",
    "        toppath = os.path.join(directory, label)\n",
    "        for filename in os.listdir(toppath):\n",
    "            #print filename\n",
    "            path = os.path.join(toppath, filename)\n",
    "            data = open(path).read().lower()\n",
    "            counter=featurizer(data)\n",
    "            totalCounts+=counter\n",
    "            docs[filename] = counter\n",
    "            labels[filename]=label\t\n",
    "\n",
    "    # set the feature featureHash\n",
    "    featureCount=0\n",
    "    for (word, count) in totalCounts.most_common(maxVocab):\n",
    "        if count >= minCount:\n",
    "            featureHash[word]=featureCount\n",
    "            featureNames.append(word)\n",
    "            featureCount+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    numericDocs={}\n",
    "    for filename in docs:\n",
    "        numericFeats={}\n",
    "        for w in docs[filename]:\n",
    "            if w in featureHash:\n",
    "                numericFeats[featureHash[w]]=1\n",
    "        numericDocs[filename]=numericFeats\n",
    "\n",
    "    train={}\n",
    "    dev={}\n",
    "\n",
    "    # split the data into 90% training, 10% development\n",
    "    i=0\n",
    "    for filename in numericDocs:\n",
    "        if i % 10 == 9:\n",
    "            dev[filename]=numericDocs[filename]\n",
    "        else:\n",
    "            train[filename]=numericDocs[filename]\n",
    "        i+=1\n",
    "\n",
    "    return (train, dev, featureNames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearKernel(x1, x2):\n",
    "    val=0.\n",
    "    for x in x1:\n",
    "        if x in x2:\n",
    "            val+=x1[x]*x2[x]\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # path to input directory containing training data\n",
    "#    directory=\"../data/movie_reviews/txt_sentoken\"\n",
    "    directory=\"../data/movie_reviews/sample100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # train and dev are both maps from filename -> dict of feature ids/values\n",
    "    # featurenNames = array of feature names indexed by feature id\n",
    "    (train, dev, featureNames, labels) = getData(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    F=len(featureNames)\n",
    "    alphas={}\n",
    "\n",
    "    values={}\n",
    "    values[\"pos\"]=1\n",
    "    values[\"neg\"]=-1\n",
    "\n",
    "    eta=1.0\n",
    "    trainN=len(train)\n",
    "    devN=len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.54444 (180), development accuracy: 0.70000 (20)\n",
      "training accuracy: 0.84444 (180), development accuracy: 0.90000 (20)\n",
      "training accuracy: 0.98333 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 0.98889 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 0.98889 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 1.00000 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 1.00000 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 1.00000 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 1.00000 (180), development accuracy: 0.80000 (20)\n",
      "training accuracy: 1.00000 (180), development accuracy: 0.80000 (20)\n"
     ]
    }
   ],
   "source": [
    "    for filename in train:\n",
    "        alphas[filename]=0\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        # train perceptron on training splits\n",
    "\n",
    "        incorrect=0.\n",
    "        n=0\n",
    "        for filename in train:\n",
    "            n+=1\n",
    " \n",
    "            val=0\n",
    "\n",
    "            # calculate the dot product \n",
    "                        \n",
    "            for comp in train:\n",
    "                val+=alphas[comp]*values[labels[comp]]*linearKernel(train[filename], train[comp])\n",
    "\n",
    "            # make prediction\n",
    "            prediction=-1\n",
    "            if val >= 0:\n",
    "                prediction=1\n",
    "\n",
    "\n",
    "            # update weights if incorrect prediction\n",
    "            trueLabel=values[labels[filename]]\n",
    "            if prediction != trueLabel:\n",
    "                alphas[filename]+=1\n",
    "                incorrect+=1\n",
    "\n",
    "\n",
    "        trainingAcc=(trainN-incorrect)/trainN\n",
    "\n",
    "        # evaluate perceptron on development splits\n",
    "\n",
    "        incorrect=0.\n",
    "\n",
    "        for filename in dev:\n",
    "            val=0.\n",
    "            \n",
    "            for comp in train:\n",
    "                val+=alphas[comp]*values[labels[comp]]*linearKernel(dev[filename], train[comp])\n",
    "          \n",
    "                \n",
    "            prediction=-1\n",
    "            \n",
    "            if val >= 0:\n",
    "                prediction=1\n",
    "                \n",
    "            trueLabel=values[labels[filename]]\n",
    "\n",
    "            if prediction != trueLabel:\n",
    "                incorrect+=1\n",
    "\n",
    "        devAcc=(devN-incorrect)/devN\n",
    "\n",
    "        print \"training accuracy: %.5f (%s), development accuracy: %.5f (%s)\" % (trainingAcc, trainN, devAcc, devN)\n",
    "\n",
    "        # end training if perfect training accuracy\n",
    "        if trainingAcc == 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
