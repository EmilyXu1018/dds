{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\"\"\" Binary classification with the perceptron\n",
    "\n",
    "Data: Cornell movie review polarity dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "values={}\n",
    "values[\"pos\"]=1\n",
    "values[\"neg\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# represent text as a set of features\n",
    "def featurizer(data):\n",
    "    # movie data is already tokenized\n",
    "    tokens = data.split(\" \")\n",
    "    counter=Counter()\n",
    "    \n",
    "    # add all features here\n",
    "    counter+=unigramFeatures(tokens)\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary indicators for all words present in text\n",
    "def unigramFeatures(tokens):\n",
    "    counter=Counter()\n",
    "    for t in tokens:\n",
    "        # binary indicators\n",
    "        counter[\"UNIGRAM:%s\" % t]=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for an input directory with {pos,neg} subdirectories, read through each file and tranform into a set of features;\n",
    "# split all data into 90% training and 10% development\n",
    "\n",
    "def getData(directory):\n",
    "    # observation parameters (minimum count for a word to be a feature, max number of total features)\n",
    "    maxVocab=10000\n",
    "    minCount=3\n",
    "\n",
    "    docs = {}\n",
    "    labels = {}\n",
    "    totalCounts=Counter()\n",
    "\n",
    "    featureHash={}\n",
    "    featureNames=[]\n",
    "\n",
    "    # read training data and get feature counts and labels for all documents\n",
    "    for label in ['pos', 'neg']:\n",
    "        toppath = os.path.join(directory, label)\n",
    "        for filename in os.listdir(toppath):\n",
    "            #print filename\n",
    "            path = os.path.join(toppath, filename)\n",
    "            data = open(path).read().lower()\n",
    "            counter=featurizer(data)\n",
    "            totalCounts+=counter\n",
    "            docs[filename] = counter\n",
    "            labels[filename]=label\t\n",
    "\n",
    "    # set the feature featureHash\n",
    "    featureCount=0\n",
    "    for (word, count) in totalCounts.most_common(maxVocab):\n",
    "        if count >= minCount:\n",
    "            featureHash[word]=featureCount\n",
    "            featureNames.append(word)\n",
    "            featureCount+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    numericDocs={}\n",
    "    for filename in docs:\n",
    "        numericFeats={}\n",
    "        for w in docs[filename]:\n",
    "            if w in featureHash:\n",
    "                numericFeats[featureHash[w]]=1\n",
    "        numericDocs[filename]=numericFeats\n",
    "\n",
    "    train={}\n",
    "    dev={}\n",
    "\n",
    "    # split the data into 90% training, 10% development\n",
    "    i=0\n",
    "    for filename in numericDocs:\n",
    "        if i % 10 == 9:\n",
    "            dev[filename]=numericDocs[filename]\n",
    "        else:\n",
    "            train[filename]=numericDocs[filename]\n",
    "        i+=1\n",
    "\n",
    "    return (train, dev, featureNames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(one, two):\n",
    "    return jaccard(one,two)\n",
    "#    return cosine(one,two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(one, two):\n",
    "    onekeys=set(one.keys())\n",
    "    twokeys=set(two.keys())\n",
    "    return float(len(set.intersection(onekeys, twokeys)))/len(set.union(onekeys, twokeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(one, two):\n",
    "    \n",
    "    sim=0.\n",
    "    norm1=0.\n",
    "    norm2=0.\n",
    "    for key in one:\n",
    "        norm1+=one[key]*one[key]\n",
    "    for key in two:\n",
    "        norm2+=two[key]*two[key]\n",
    "        \n",
    "    for key in one:\n",
    "        if key in two:\n",
    "            sim+=one[key] * two[key]\n",
    "    return sim/(sqrt(norm1) * sqrt(norm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KNN(datapoint, train, labels, K):\n",
    "    scores={}\n",
    "    for trainingPoint in train:\n",
    "        score=sim(datapoint, train[trainingPoint])\n",
    "        scores[trainingPoint]=score\n",
    "        \n",
    "    sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "   \n",
    "    counts=np.zeros(2)\n",
    "    \n",
    "    if K >= len(scores):\n",
    "        K=len(scores)\n",
    "        \n",
    "    for i in range(K):\n",
    "        (k,v)=sorted_x[i]\n",
    "        label=values[labels[k]]\n",
    "        counts[label]+=1\n",
    "    \n",
    "    return np.argmax(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # path to input directory containing training data\n",
    "    directory=\"../data/movie_reviews/sample100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # train and dev are both maps from filename -> dict of feature ids/values\n",
    "    # featurenNames = array of feature names indexed by feature id\n",
    "    (train, dev, featureNames, labels) = getData(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . \n",
      "Accuracy: 0.850, (17/20)\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "\n",
    "# K nearest neighbors\n",
    "K=3\n",
    "\n",
    "for filename in dev:\n",
    "    prediction=KNN(dev[filename], train, labels, K)\n",
    "                                 \n",
    "    if prediction == values[labels[filename]]:\n",
    "        correct+=1\n",
    "    total+=1\n",
    "\n",
    "    if total % 5 == 0:\n",
    "        print \".\",\n",
    "\n",
    "print \"\\nAccuracy: %.3f, (%s/%s)\" % (float(correct)/total, correct, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
