{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.special import beta\n",
    "from scipy.special import gamma, gammaln, digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# represent text as a set of features\n",
    "def featurizer(data):\n",
    "    # movie data is already tokenized\n",
    "    tokens = data.split()\n",
    "    counter=Counter()\n",
    "    for t in tokens:\n",
    "        counter[t]+=1\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(directory):\n",
    "    # observation parameters (minimum count for a word to be a feature, max number of total features)\n",
    "    maxVocab=10000\n",
    "    minCount=3\n",
    "\n",
    "    docs = {}\n",
    "    labels = {}\n",
    "    totalCounts=Counter()\n",
    "\n",
    "    featureHash={}\n",
    "    featureNames=[]\n",
    "\n",
    "    # read training data and get feature counts and labels for all documents\n",
    "    for label in ['pos', 'neg']:\n",
    "        toppath = os.path.join(directory, label)\n",
    "        for filename in os.listdir(toppath):\n",
    "            #print filename\n",
    "            path = os.path.join(toppath, filename)\n",
    "            data = open(path).read().lower()\n",
    "            counter=featurizer(data)\n",
    "            totalCounts+=counter\n",
    "            docs[filename] = counter\n",
    "            labels[filename]=label\n",
    "\n",
    "    # set the feature featureHash\n",
    "    featureCount=0\n",
    "    for (word, count) in totalCounts.most_common(maxVocab):\n",
    "        if count >= minCount:\n",
    "            featureHash[word]=featureCount\n",
    "            featureNames.append(word)\n",
    "            featureCount+=1\n",
    "\n",
    "    # represent documents as only those words in the common vocabulary (everything else is thrown away)\n",
    "    numericDocs={}\n",
    "    for filename in docs:\n",
    "        numericFeats=Counter()\n",
    "        for w in docs[filename]:\n",
    "            if w in featureHash:\n",
    "                numericFeats[featureHash[w]]=docs[filename][w]\n",
    "\n",
    "        numericDocs[filename]=numericFeats\n",
    "\n",
    "    train={}\n",
    "    dev={}\n",
    "\n",
    "    # split the data into 90% training, 10% development\n",
    "    i=0\n",
    "    for filename in numericDocs:\n",
    "        if i % 10 == 9:\n",
    "            dev[filename]=numericDocs[filename]\n",
    "        else:\n",
    "            train[filename]=numericDocs[filename]\n",
    "        i+=1\n",
    "\n",
    "    return (train, dev, featureNames, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multinomial naive Bayes models the feature *counts* as multinomials (so that how often a feature shows up with\n",
    "# data is important)\n",
    "def multinomialNB(train, dev, featureNames, labels):\n",
    "    F=len(featureNames)\n",
    "    values={}\n",
    "    values[\"pos\"]=0\n",
    "    values[\"neg\"]=1\n",
    "\n",
    "    # smooth priors [p(y)] and word likelihoods [p(x|y)]\n",
    "    priors=np.ones(2, dtype=float)*1.0\n",
    "    word_likelihoods=np.ones((2, F))*1.0\n",
    "\n",
    "    # estimate parameters\n",
    "    for filename in train:\n",
    "        label=values[labels[filename]]\n",
    "        for word in train[filename]:\n",
    "            word_likelihoods[label][word]+=train[filename][word]\n",
    "\n",
    "        priors[label]+=1.\n",
    "\n",
    "    priors/=np.sum(priors)\n",
    "    \n",
    "    orig=np.ones((2, F))\n",
    "    orig[0]=list(word_likelihoods[0])\n",
    "    orig[1]=list(word_likelihoods[1])\n",
    "    \n",
    "\n",
    "    word_likelihoods[0]/=np.sum(word_likelihoods[0])\n",
    "    word_likelihoods[1]/=np.sum(word_likelihoods[1])\n",
    "\n",
    "\n",
    "    # make predictions\n",
    "    incorrect=0.\n",
    "\n",
    "    # use log to avoid numerical underflow (when multiplying many probabilities together)\n",
    "    for filename in dev:\n",
    "        class0=math.log(priors[0])\n",
    "        class1=math.log(priors[1])\n",
    "\n",
    "        for w in dev[filename]:\n",
    "            class0+=dev[filename][w]*math.log(word_likelihoods[0][w])\n",
    "            class1+=dev[filename][w]*math.log(word_likelihoods[1][w])\n",
    "\n",
    "        trueLabel=values[labels[filename]]\n",
    "\n",
    "        # prediction is whichever class has the highest probability\n",
    "        prediction=0\n",
    "        if class1 > class0:\n",
    "            prediction=1\n",
    "        if prediction != trueLabel:\n",
    "            incorrect+=1\n",
    "\n",
    "\n",
    "    print \"Accuracy: %.3f\" % (1-incorrect/len(dev))\n",
    "\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bernoulli Naive Bayes models each feature as an independent bernoulli random variable (a binary value of present or absent)\n",
    "\n",
    "def bernoulliNB(train, dev, featureNames, labels):\n",
    "    F=len(featureNames)\n",
    "    values={}\n",
    "    values[\"pos\"]=0\n",
    "    values[\"neg\"]=1\n",
    "\n",
    "    # smooth priors [p(y)] and word likelihoods [p(x|y)]\n",
    "    priors=np.ones(2, dtype=float)*1.0\n",
    "    word_likelihoods=np.ones((2, F))*1.0\n",
    "\n",
    "    # estimate parameters (count number of docs containing each term in vocabulary)\n",
    "    for filename in train:\n",
    "        label=values[labels[filename]]\n",
    "        for word in train[filename]:\n",
    "            word_likelihoods[label][word]+=1\n",
    "\n",
    "        priors[label]+=1.\n",
    "\n",
    "    # normalize each probability distribution\n",
    "    priors/=np.sum(priors)\n",
    "    word_likelihoods[0]/=len(train)\n",
    "    word_likelihoods[1]/=len(train)\n",
    "\n",
    "\n",
    "    # make predictions\n",
    "    incorrect=0.\n",
    "\n",
    "    # use log to avoid numerical underflow (when multiplying many probabilities together)\n",
    "    for filename in dev:\n",
    "        class0=math.log(priors[0])\n",
    "        class1=math.log(priors[1])\n",
    "\n",
    "        for i in range(F):\n",
    "            if i in dev[filename]:\n",
    "                class0+=math.log(word_likelihoods[0][i])\n",
    "                class1+=math.log(word_likelihoods[1][i])\n",
    "            else:\n",
    "                class0+=math.log(1-word_likelihoods[0][i])\n",
    "                class1+=math.log(1-word_likelihoods[1][i])\n",
    "\n",
    "\n",
    "        trueLabel=values[labels[filename]]\n",
    "\n",
    "        # prediction is whichever class has the highest probability\n",
    "        prediction=0\n",
    "        if class1 > class0:\n",
    "            prediction=1\n",
    "        if prediction != trueLabel:\n",
    "            incorrect+=1\n",
    "\n",
    "\n",
    "    print \"Accuracy: %.3f\" % (1-incorrect/len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printFreqs(counts, symmetricAlpha, display):\n",
    "\n",
    "    F=len(counts[0])\n",
    "\n",
    "    freqs=np.zeros((2, F))\n",
    "    freqs[0]=np.array(counts[0])+symmetricAlpha\n",
    "    freqs[1]=np.array(counts[1])+symmetricAlpha\n",
    "    \n",
    "    freqs[0]/=np.sum(freqs[0])\n",
    "    freqs[1]/=np.sum(freqs[1])\n",
    "  \n",
    "    zipped1=zip(freqs[0], featureNames)                     # zip two lists together to iterate through them simultaneously\n",
    "    zipped1.sort(key = lambda t: t[0], reverse=True)     # sort the two lists by the values in the first (the coefficients)\n",
    "\n",
    "    zipped2=zip(freqs[1], featureNames)                     # zip two lists together to iterate through them simultaneously\n",
    "    zipped2.sort(key = lambda t: t[0], reverse=True)     # sort the two lists by the values in the first (the coefficients)\n",
    "\n",
    "\n",
    "    print \"\\n## ABSOLUTE FREQUENCIES ##\"\n",
    "    print \"%10s\\t\\t%10s\" % (\"\\nCLASS 1:\", \"CLASS 2:\")\n",
    "    for i in range(display):\n",
    "        (weight, word) = zipped1[i]\n",
    "        (weight2, word2) = zipped2[i]\n",
    "        \n",
    "        print \"%.5f\\t%10s\\t%.5f\\t%10s\" % (weight, word, weight2, word2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printFrequencyRatio(counts, symmetricAlpha, display):\n",
    "    \n",
    "    F=len(counts[0])\n",
    "    freqs=np.zeros((2, F))\n",
    "    freqs[0]=np.array(counts[0])+symmetricAlpha\n",
    "    freqs[1]=np.array(counts[1])+symmetricAlpha\n",
    "    \n",
    "    freqs[0]/=np.sum(freqs[0])\n",
    "    freqs[1]/=np.sum(freqs[1])\n",
    "  \n",
    "    diff=np.zeros(F)\n",
    "    for i in range(len(diff)):\n",
    "        diff[i]=math.log(freqs[0][i]/freqs[1][i])\n",
    "    \n",
    "    zipped1=zip(diff, featureNames)                     # zip two lists together to iterate through them simultaneously\n",
    "    zipped1.sort(key = lambda t: t[0], reverse=True)     # sort the two lists by the values in the first (the coefficients)\n",
    "\n",
    "\n",
    "    print \"\\n## FREQUENCY RATIO ##\"\n",
    "    print \"%10s\\t\\t%10s\" % (\"\\nCLASS 1:\", \"CLASS 2:\")\n",
    "    for i in range(display):\n",
    "        (weight, word) = zipped1[i]\n",
    "        (weight2, word2) = zipped1[len(zipped1)-i-1]\n",
    "        \n",
    "        print \"%.5f\\t%10s\\t%.5f\\t%10s\" % (weight, word, weight2, word2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smoothed log-odds ratio, from:\n",
    "# Monroe et al. (2009), \"Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict\"\n",
    "# http://languagelog.ldc.upenn.edu/myl/Monroe.pdf\n",
    "def printLogOddsRatio(counts, symmetricAlpha, display):\n",
    "    \n",
    "    F=len(counts[0])\n",
    "\n",
    "    freqs=np.zeros((2, F))\n",
    "    freqs[0]=np.array(counts[0])\n",
    "    freqs[1]=np.array(counts[1])\n",
    "    \n",
    "    freqs[0]/=np.sum(freqs[0])\n",
    "    freqs[1]/=np.sum(freqs[1])\n",
    "    \n",
    "    diffs=np.zeros(F)\n",
    "    sum0=np.sum(counts[0])\n",
    "    sum1=np.sum(counts[1])\n",
    "    \n",
    "    for i in range(F):\n",
    "        diffs[i]=math.log((counts[0][i] + symmetricAlpha)/(sum0 + F*symmetricAlpha - counts[0][i] - symmetricAlpha)) - math.log((counts[1][i] + symmetricAlpha)/(sum1 + F - counts[1][i] - symmetricAlpha))\n",
    "\n",
    "    zipped=zip(diffs, featureNames)                     # zip two lists together to iterate through them simultaneously\n",
    "    zipped.sort(key = lambda t: t[0], reverse=True)     # sort the two lists by the values in the first (the coefficients)\n",
    "\n",
    "    print \"\\n## LOG ODDS ##\"\n",
    "    print \"%10s\\t\\t%10s\" % (\"\\nCLASS 1:\", \"CLASS 2:\")\n",
    "    for i in range(display):\n",
    "        (weight, word) = zipped[i]\n",
    "        (weight2, word2) = zipped[len(zipped)-i-1]\n",
    "        \n",
    "        print \"%.3f\\t%10s\\t%.3f\\t%10s\" % (weight, word, weight2, word2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smoothed log-odds ratio accounting for variance, from:\n",
    "# Monroe et al. (2009), \"Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict\"\n",
    "# http://languagelog.ldc.upenn.edu/myl/Monroe.pdf\n",
    "def printLogOddsRatioWVariance(counts, symmetricAlpha, display):\n",
    "    \n",
    "    F=len(counts[0])\n",
    "\n",
    "    freqs=np.zeros((2, F))\n",
    "    freqs[0]=np.array(counts[0])\n",
    "    freqs[1]=np.array(counts[1])\n",
    "    \n",
    "    freqs[0]/=np.sum(freqs[0])\n",
    "    freqs[1]/=np.sum(freqs[1])\n",
    "    \n",
    "    diffs=np.zeros(F)\n",
    "    sum0=np.sum(counts[0])\n",
    "    sum1=np.sum(counts[1])\n",
    "    \n",
    "    for i in range(F):\n",
    "        diffs[i]=math.log((counts[0][i] + symmetricAlpha)/(sum0 + F*symmetricAlpha - counts[0][i] - symmetricAlpha)) - math.log((counts[1][i] + symmetricAlpha)/(sum1 + F - counts[1][i] - symmetricAlpha))\n",
    "        diffs[i]/=math.sqrt( (1./(counts[0][i] + symmetricAlpha)) + (1./(counts[1][i] + symmetricAlpha))  )\n",
    "\n",
    "    zipped=zip(diffs, featureNames)                     # zip two lists together to iterate through them simultaneously\n",
    "    zipped.sort(key = lambda t: t[0], reverse=True)     # sort the two lists by the values in the first (the coefficients)\n",
    "\n",
    "    print \"\\n## LOG ODDS WITH VARIANCE ##\"\n",
    "    print \"%10s\\t\\t%10s\" % (\"\\nCLASS 1:\", \"CLASS 2:\")\n",
    "    for i in range(display):\n",
    "        (weight, word) = zipped[i]\n",
    "        (weight2, word2) = zipped[len(zipped)-i-1]\n",
    "        \n",
    "        print \"%.3f\\t%10s\\t%.3f\\t%10s\" % (weight, word, weight2, word2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # path to input directory containing training data\n",
    "    directory=\"../data/movie_reviews/txt_sentoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # train and dev are both maps from filename -> dict of feature ids/values\n",
    "    # featurenNames = array of feature names indexed by feature id\n",
    "    (train, dev, featureNames, labels) = getData(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.800\n"
     ]
    }
   ],
   "source": [
    "    bernoulliNB(train, dev, featureNames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "    counts=multinomialNB(train, dev, featureNames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## ABSOLUTE FREQUENCIES ##\n",
      " \n",
      "CLASS 1:\t\t  CLASS 2:\n",
      "0.05581\t         ,\t0.05151\t         ,\n",
      "0.05425\t       the\t0.05070\t       the\n",
      "0.04438\t         .\t0.04708\t         .\n",
      "0.02626\t         a\t0.02600\t         a\n",
      "0.02601\t       and\t0.02267\t       and\n",
      "0.02439\t        of\t0.02258\t        of\n",
      "0.02165\t        to\t0.02235\t        to\n",
      "0.01851\t        is\t0.01626\t        is\n",
      "0.01526\t        in\t0.01457\t        in\n",
      "0.01137\t         \"\t0.01338\t         \"\n"
     ]
    }
   ],
   "source": [
    "    # print three different views of the learned model\n",
    "    printFreqs(counts, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## FREQUENCY RATIO ##\n",
      " \n",
      "CLASS 1:\t\t  CLASS 2:\n",
      "3.15373\t     mulan\t-3.51830\t     &nbsp\n",
      "3.03989\t     shrek\t-3.17738\t    seagal\n",
      "2.94316\t     flynt\t-2.85795\t   brenner\n",
      "2.90332\t   gattaca\t-2.75616\t       8mm\n",
      "2.74509\t    ordell\t-2.68206\t    sphere\n",
      "2.62373\t     leila\t-2.55945\t      1900\n",
      "2.55704\t  truman's\t-2.51500\t       bye\n",
      "2.55704\t sweetback\t-2.51500\t   pokemon\n",
      "2.52195\t     taran\t-2.49201\tschumacher\n",
      "2.48558\t     guido\t-2.48423\tjawbreaker\n"
     ]
    }
   ],
   "source": [
    "    printFrequencyRatio(counts, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## LOG ODDS ##\n",
      " \n",
      "CLASS 1:\t\t  CLASS 2:\n",
      "3.154\t     mulan\t-3.518\t     &nbsp\n",
      "3.040\t     shrek\t-3.177\t    seagal\n",
      "2.943\t     flynt\t-2.858\t   brenner\n",
      "2.903\t   gattaca\t-2.756\t       8mm\n",
      "2.745\t    ordell\t-2.682\t    sphere\n",
      "2.624\t     leila\t-2.559\t      1900\n",
      "2.557\t  truman's\t-2.515\t       bye\n",
      "2.557\t sweetback\t-2.515\t   pokemon\n",
      "2.522\t     taran\t-2.492\tschumacher\n",
      "2.486\t     guido\t-2.484\tjawbreaker\n"
     ]
    }
   ],
   "source": [
    "    printLogOddsRatio(counts, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## LOG ODDS WITH VARIANCE ##\n",
      " \n",
      "CLASS 1:\t\t  CLASS 2:\n",
      "12.477\t       and\t-18.038\t       bad\n",
      "11.187\t         ,\t-12.050\t         ?\n",
      "10.596\t       his\t-11.530\t     movie\n",
      "10.108\t      life\t-10.799\t         i\n",
      "9.884\t        is\t-10.605\t         *\n",
      "9.325\t       the\t-10.474\t         !\n",
      "9.192\t        as\t-10.452\t         \"\n",
      "8.764\t         =\t-10.446\t     worst\n",
      "8.147\t     great\t-9.481\t    stupid\n",
      "7.965\t       war\t-9.303\t    boring\n"
     ]
    }
   ],
   "source": [
    "    printLogOddsRatioWVariance(counts, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
