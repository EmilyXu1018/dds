{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Binary classification with the perceptron '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, nltk\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" Binary classification with the perceptron\n",
    "\n",
    "Data: Cornell movie review polarity dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# represent text as a set of features\n",
    "def featurizer(data):\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    counter=Counter()\n",
    "    \n",
    "    # add all features here\n",
    "    counter+=unigramFeatures(tokens)\n",
    "    counter+=biasTerm()\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary indicators for all words present in text\n",
    "def unigramFeatures(tokens):\n",
    "    counter=Counter()\n",
    "    for t in tokens:\n",
    "        # binary indicators\n",
    "        counter[\"UNIGRAM:%s\" % t]=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bias term\n",
    "def biasTerm():\n",
    "    counter=Counter()\n",
    "    counter[\"BIAS\"]=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for an input directory with {pos,neg} subdirectories, read through each file and tranform into a set of features;\n",
    "# split all data into 90% training and 10% development\n",
    "\n",
    "def getData(directory):\n",
    "    # observation parameters (minimum count for a word to be a feature, max number of total features)\n",
    "    maxVocab=10000\n",
    "    minCount=3\n",
    "\n",
    "    docs = {}\n",
    "    labels = {}\n",
    "    totalCounts=Counter()\n",
    "\n",
    "    featureHash={}\n",
    "    featureNames=[]\n",
    "\n",
    "    # read training data and get feature counts and labels for all documents\n",
    "    for label in ['pos', 'neg']:\n",
    "        toppath = os.path.join(directory, label)\n",
    "        for filename in os.listdir(toppath):\n",
    "            #print filename\n",
    "            path = os.path.join(toppath, filename)\n",
    "            data = open(path).read().lower()\n",
    "            counter=featurizer(data)\n",
    "            totalCounts+=counter\n",
    "            docs[filename] = counter\n",
    "            labels[filename]=label\t\n",
    "\n",
    "    # set the feature featureHash\n",
    "    featureCount=0\n",
    "    for (word, count) in totalCounts.most_common(maxVocab):\n",
    "        if count >= minCount:\n",
    "            featureHash[word]=featureCount\n",
    "            featureNames.append(word)\n",
    "            featureCount+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    numericDocs={}\n",
    "    for filename in docs:\n",
    "        numericFeats={}\n",
    "        for w in docs[filename]:\n",
    "            if w in featureHash:\n",
    "                numericFeats[featureHash[w]]=1\n",
    "        numericDocs[filename]=numericFeats\n",
    "\n",
    "    train={}\n",
    "    dev={}\n",
    "\n",
    "    # split the data into 90% training, 10% development\n",
    "    i=0\n",
    "    for filename in numericDocs:\n",
    "        if i % 10 == 9:\n",
    "            dev[filename]=numericDocs[filename]\n",
    "        else:\n",
    "            train[filename]=numericDocs[filename]\n",
    "        i+=1\n",
    "\n",
    "    return (train, dev, featureNames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # input directory containing training data\n",
    "\n",
    "    #directory=\"/Users/dbamman/Downloads/review_polarity/bigtest\"\n",
    "    directory=\"/Users/dbamman/Downloads/review_polarity/txt_sentoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # train and dev are both maps from filename -> dict of feature ids/values\n",
    "    # featurenNames = array of feature names indexed by feature id\n",
    "    (train, dev, featureNames, labels) = getData(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    F=len(featureNames)\n",
    "    betas=np.zeros(F)\n",
    "\n",
    "    values={}\n",
    "    values[\"pos\"]=1\n",
    "    values[\"neg\"]=-1\n",
    "\n",
    "    eta=1.0\n",
    "    trainN=len(train)\n",
    "    devN=len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.72056 (1800), development accuracy: 0.80500 (200)\n",
      "training accuracy: 0.91111 (1800), development accuracy: 0.81000 (200)\n",
      "training accuracy: 0.96056 (1800), development accuracy: 0.80500 (200)\n",
      "training accuracy: 0.96944 (1800), development accuracy: 0.83000 (200)\n",
      "training accuracy: 0.98500 (1800), development accuracy: 0.83000 (200)\n",
      "training accuracy: 0.99222 (1800), development accuracy: 0.81500 (200)\n",
      "training accuracy: 0.99333 (1800), development accuracy: 0.83000 (200)\n",
      "training accuracy: 0.99667 (1800), development accuracy: 0.83500 (200)\n",
      "training accuracy: 1.00000 (1800), development accuracy: 0.83500 (200)\n"
     ]
    }
   ],
   "source": [
    "    for i in range(100):\n",
    "\n",
    "        # train perceptron on training splits\n",
    "\n",
    "        incorrect=0.\n",
    "        for filename in train:\n",
    "            val=0\n",
    "\n",
    "            # calculate the dot product \n",
    "            for i in train[filename]:\n",
    "                val+=betas[i]*train[filename][i]\n",
    "\n",
    "            # make prediction\n",
    "            prediction=-1\n",
    "            if val >= 0:\n",
    "                prediction=1\n",
    "\n",
    "\n",
    "            # update weights if incorrect prediction\n",
    "            trueLabel=values[labels[filename]]\n",
    "            if prediction != trueLabel:\n",
    "                incorrect+=1\n",
    "                for i in train[filename]:\n",
    "                    betas[i]+=eta * trueLabel * train[filename][i]\n",
    "\n",
    "\n",
    "        trainingAcc=(trainN-incorrect)/trainN\n",
    "\n",
    "        # evaluate perceptron on development splits\n",
    "\n",
    "        incorrect=0.\n",
    "\n",
    "        for filename in dev:\n",
    "            val=0.\n",
    "            for i in dev[filename]:\n",
    "                val+=betas[i]*dev[filename][i]\n",
    "            prediction=-1\n",
    "            \n",
    "            if val >= 0:\n",
    "                prediction=1\n",
    "                \n",
    "            trueLabel=values[labels[filename]]\n",
    "\n",
    "            if prediction != trueLabel:\n",
    "                incorrect+=1\n",
    "\n",
    "        devAcc=(devN-incorrect)/devN\n",
    "\n",
    "        print \"training accuracy: %.5f (%s), development accuracy: %.5f (%s)\" % (trainingAcc, trainN, devAcc, devN)\n",
    "\n",
    "        # end training if perfect training accuracy\n",
    "        if trainingAcc == 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipped=zip(betas, featureNames)\t\t\t# zip two lists together to iterate through them simultaneously\n",
    "zipped.sort(key = lambda t: t[0], reverse=True)\t\t# sort the two lists by the values in the first (the coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST POSITIVE FEATURES:\n",
      "23.000\tUNIGRAM:memorable\n",
      "23.000\tUNIGRAM:terrific\n",
      "22.000\tUNIGRAM:see\n",
      "22.000\tUNIGRAM:hilarious\n",
      "21.000\tUNIGRAM:best\n",
      "21.000\tUNIGRAM:both\n",
      "21.000\tUNIGRAM:change\n",
      "21.000\tUNIGRAM:flaws\n",
      "20.000\tUNIGRAM:many\n",
      "20.000\tUNIGRAM:less\n",
      "\n",
      "MOST NEGATIVE FEATURES:\n",
      "-35.000\tUNIGRAM:bad\n",
      "-32.000\tUNIGRAM:nothing\n",
      "-32.000\tUNIGRAM:plot\n",
      "-30.000\tUNIGRAM:worst\n",
      "-28.000\tUNIGRAM:only\n",
      "-27.000\tUNIGRAM:boring\n",
      "-25.000\tUNIGRAM:unfortunately\n",
      "-24.000\tUNIGRAM:ridiculous\n",
      "-24.000\tUNIGRAM:script\n"
     ]
    }
   ],
   "source": [
    "print \"MOST POSITIVE FEATURES:\"\n",
    "for (weight, word) in zipped[:10]:\n",
    "    print \"%.3f\\t%s\" % (weight, word)\n",
    "\n",
    "print \"\\nMOST NEGATIVE FEATURES:\"\n",
    "for (weight, word) in zipped[:-10:-1]:\n",
    "    print \"%.3f\\t%s\" % (weight, word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write weights\n",
    "\n",
    "out=open(\"weights.txt\", \"w\")\n",
    "for (weight, word) in zipped:\n",
    "    out.write(\"%.5f\\t%s\\n\" % (weight, word))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
